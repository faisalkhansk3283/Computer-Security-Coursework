{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "2 a), 2 b)"
      ],
      "metadata": {
        "id": "cdBUDFNPChQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Split the full training set into a smaller training set and a validation set\n",
        "x_train, x_valid = x_train_full[:50000] , x_train_full[50000:] \n",
        "y_train, y_valid = y_train_full[:50000], y_train_full[50000:]\n",
        "\n",
        "# Load MNIST data and normalize it\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_valid = x_valid.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Flatten the images to a 784-dimensional vector\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_valid = x_valid.reshape((len(x_valid), np.prod(x_valid.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# Define the autoencoder model\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(128, activation='relu')(input_img)\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the autoencoder on MNIST data for 50 epochs with batch size of 256\n",
        "history = autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=False, validation_data=(x_valid, x_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTwwP9qN2LK5",
        "outputId": "949d07b2-a9e8-4882-a8a6-dbede0087885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "196/196 [==============================] - 4s 17ms/step - loss: 0.2395 - val_loss: 0.1597\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.1409 - val_loss: 0.1245\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 3s 17ms/step - loss: 0.1162 - val_loss: 0.1076\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.1029 - val_loss: 0.0981\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0942 - val_loss: 0.0907\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0876 - val_loss: 0.0862\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0829 - val_loss: 0.0819\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0794 - val_loss: 0.0784\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0769 - val_loss: 0.0764\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0751 - val_loss: 0.0748\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0737 - val_loss: 0.0735\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0725 - val_loss: 0.0726\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 4s 19ms/step - loss: 0.0716 - val_loss: 0.0718\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 4s 19ms/step - loss: 0.0709 - val_loss: 0.0711\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0703 - val_loss: 0.0705\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0697 - val_loss: 0.0701\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0693 - val_loss: 0.0697\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0689 - val_loss: 0.0693\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0686 - val_loss: 0.0691\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0683 - val_loss: 0.0688\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0681 - val_loss: 0.0686\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0679 - val_loss: 0.0684\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0677 - val_loss: 0.0682\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0675 - val_loss: 0.0680\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0674 - val_loss: 0.0679\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0672 - val_loss: 0.0677\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0671 - val_loss: 0.0676\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0670 - val_loss: 0.0675\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 4s 19ms/step - loss: 0.0669 - val_loss: 0.0674\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 4s 18ms/step - loss: 0.0668 - val_loss: 0.0673\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0667 - val_loss: 0.0672\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0666 - val_loss: 0.0672\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0666 - val_loss: 0.0671\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0665 - val_loss: 0.0670\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0664 - val_loss: 0.0670\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0664 - val_loss: 0.0669\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0663 - val_loss: 0.0669\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0663 - val_loss: 0.0668\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 4s 19ms/step - loss: 0.0662 - val_loss: 0.0668\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 4s 19ms/step - loss: 0.0662 - val_loss: 0.0667\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0661 - val_loss: 0.0667\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0661 - val_loss: 0.0667\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0661 - val_loss: 0.0666\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0660 - val_loss: 0.0666\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0660 - val_loss: 0.0666\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0660 - val_loss: 0.0666\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0659 - val_loss: 0.0665\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0659 - val_loss: 0.0665\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0659 - val_loss: 0.0665\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0659 - val_loss: 0.0665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Split the full training set into a smaller training set and a validation set\n",
        "x_train, x_valid = x_train_full[:50000] , x_train_full[50000:] \n",
        "y_train, y_valid = y_train_full[:50000], y_train_full[50000:]\n",
        "\n",
        "# Load MNIST data and normalize it\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_valid = x_valid.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Flatten the images to a 784-dimensional vector\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_valid = x_valid.reshape((len(x_valid), np.prod(x_valid.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# Define the autoencoder model\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(256, activation='relu')(input_img)\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the autoencoder on MNIST data for 50 epochs with batch size of 256\n",
        "history = autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=False, validation_data=(x_valid, x_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v4M8DmV0N4Y",
        "outputId": "194165f0-0259-44e0-f2d1-58c02eac337f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "196/196 [==============================] - 5s 22ms/step - loss: 0.2080 - val_loss: 0.1340\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.1184 - val_loss: 0.1052\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0979 - val_loss: 0.0918\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0871 - val_loss: 0.0845\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0805 - val_loss: 0.0783\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0763 - val_loss: 0.0751\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0736 - val_loss: 0.0730\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0717 - val_loss: 0.0714\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0703 - val_loss: 0.0703\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0693 - val_loss: 0.0694\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 6s 30ms/step - loss: 0.0685 - val_loss: 0.0688\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0679 - val_loss: 0.0682\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0675 - val_loss: 0.0678\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 5s 26ms/step - loss: 0.0671 - val_loss: 0.0674\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0667 - val_loss: 0.0671\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 5s 28ms/step - loss: 0.0664 - val_loss: 0.0668\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0662 - val_loss: 0.0666\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0660 - val_loss: 0.0664\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0658 - val_loss: 0.0663\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0656 - val_loss: 0.0661\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0655 - val_loss: 0.0660\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0653 - val_loss: 0.0660\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0652 - val_loss: 0.0659\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 7s 34ms/step - loss: 0.0651 - val_loss: 0.0658\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0650 - val_loss: 0.0657\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0649 - val_loss: 0.0656\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 6s 30ms/step - loss: 0.0649 - val_loss: 0.0656\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0648 - val_loss: 0.0655\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0647 - val_loss: 0.0654\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 5s 28ms/step - loss: 0.0647 - val_loss: 0.0653\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0646 - val_loss: 0.0653\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 6s 28ms/step - loss: 0.0645 - val_loss: 0.0652\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0645 - val_loss: 0.0652\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0645 - val_loss: 0.0651\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 6s 30ms/step - loss: 0.0644 - val_loss: 0.0651\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0644 - val_loss: 0.0650\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0643 - val_loss: 0.0650\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0643 - val_loss: 0.0650\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0643 - val_loss: 0.0649\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 6s 30ms/step - loss: 0.0642 - val_loss: 0.0649\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0642 - val_loss: 0.0649\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0642 - val_loss: 0.0648\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0642 - val_loss: 0.0648\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0641 - val_loss: 0.0648\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0641 - val_loss: 0.0648\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0641 - val_loss: 0.0648\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0641 - val_loss: 0.0647\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 5s 28ms/step - loss: 0.0640 - val_loss: 0.0647\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0640 - val_loss: 0.0647\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0640 - val_loss: 0.0647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Split the full training set into a smaller training set and a validation set\n",
        "x_train, x_valid = x_train_full[:50000] , x_train_full[50000:] \n",
        "y_train, y_valid = y_train_full[:50000], y_train_full[50000:]\n",
        "\n",
        "# Load MNIST data and normalize it\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_valid = x_valid.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Flatten the images to a 784-dimensional vector\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_valid = x_valid.reshape((len(x_valid), np.prod(x_valid.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# Define the autoencoder model\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(256, activation='relu',use_bias = True)(input_img)\n",
        "decoded = Dense(784, activation='sigmoid',use_bias = True)(encoded)\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the autoencoder on MNIST data for 50 epochs with batch size of 256\n",
        "history = autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=False, validation_data=(x_valid, x_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMiS7IA9ncwl",
        "outputId": "03fbda79-7970-44e2-84f4-c847b1fe0fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "196/196 [==============================] - 5s 22ms/step - loss: 0.2078 - val_loss: 0.1337\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.1180 - val_loss: 0.1049\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0975 - val_loss: 0.0917\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0869 - val_loss: 0.0847\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0805 - val_loss: 0.0783\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 5s 26ms/step - loss: 0.0763 - val_loss: 0.0751\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0735 - val_loss: 0.0729\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0716 - val_loss: 0.0714\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0703 - val_loss: 0.0702\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 5s 26ms/step - loss: 0.0693 - val_loss: 0.0694\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 6s 28ms/step - loss: 0.0685 - val_loss: 0.0688\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0680 - val_loss: 0.0682\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0675 - val_loss: 0.0678\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 7s 35ms/step - loss: 0.0671 - val_loss: 0.0674\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 7s 34ms/step - loss: 0.0667 - val_loss: 0.0671\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 6s 31ms/step - loss: 0.0664 - val_loss: 0.0668\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0662 - val_loss: 0.0666\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 5s 28ms/step - loss: 0.0660 - val_loss: 0.0664\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0658 - val_loss: 0.0663\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0656 - val_loss: 0.0661\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 6s 28ms/step - loss: 0.0655 - val_loss: 0.0661\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0654 - val_loss: 0.0660\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 6s 30ms/step - loss: 0.0652 - val_loss: 0.0659\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0651 - val_loss: 0.0658\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0650 - val_loss: 0.0657\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 6s 30ms/step - loss: 0.0649 - val_loss: 0.0656\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0648 - val_loss: 0.0655\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0648 - val_loss: 0.0655\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0647 - val_loss: 0.0654\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0646 - val_loss: 0.0653\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0646 - val_loss: 0.0653\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0645 - val_loss: 0.0652\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0645 - val_loss: 0.0651\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 6s 30ms/step - loss: 0.0644 - val_loss: 0.0651\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0644 - val_loss: 0.0650\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0643 - val_loss: 0.0650\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 5s 26ms/step - loss: 0.0643 - val_loss: 0.0649\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0643 - val_loss: 0.0649\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 6s 28ms/step - loss: 0.0642 - val_loss: 0.0649\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0642 - val_loss: 0.0648\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0642 - val_loss: 0.0648\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0641 - val_loss: 0.0648\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0641 - val_loss: 0.0648\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0641 - val_loss: 0.0647\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0641 - val_loss: 0.0647\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0640 - val_loss: 0.0647\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0640 - val_loss: 0.0647\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0640 - val_loss: 0.0647\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0640 - val_loss: 0.0646\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 5s 28ms/step - loss: 0.0640 - val_loss: 0.0646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Split the full training set into a smaller training set and a validation set\n",
        "x_train, x_valid = x_train_full[:50000] , x_train_full[50000:] \n",
        "y_train, y_valid = y_train_full[:50000], y_train_full[50000:]\n",
        "\n",
        "# Load MNIST data and normalize it\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_valid = x_valid.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Flatten the images to a 784-dimensional vector\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_valid = x_valid.reshape((len(x_valid), np.prod(x_valid.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# Define the autoencoder model\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(256, activation='tanh')(input_img)\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the autoencoder on MNIST data for 50 epochs with batch size of 256\n",
        "history = autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_valid, x_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWcUfWjl8WHC",
        "outputId": "b4fabdda-962b-45da-fdd7-a237530bbe84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "196/196 [==============================] - 7s 32ms/step - loss: 0.2262 - val_loss: 0.1577\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.1365 - val_loss: 0.1187\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.1089 - val_loss: 0.1005\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0952 - val_loss: 0.0907\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0875 - val_loss: 0.0851\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0828 - val_loss: 0.0814\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0796 - val_loss: 0.0787\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0772 - val_loss: 0.0768\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 5s 26ms/step - loss: 0.0755 - val_loss: 0.0753\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0741 - val_loss: 0.0741\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0730 - val_loss: 0.0731\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0721 - val_loss: 0.0723\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0714 - val_loss: 0.0716\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0707 - val_loss: 0.0710\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0702 - val_loss: 0.0705\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0697 - val_loss: 0.0701\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0692 - val_loss: 0.0697\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0689 - val_loss: 0.0693\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0685 - val_loss: 0.0690\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0682 - val_loss: 0.0688\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0679 - val_loss: 0.0685\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0677 - val_loss: 0.0683\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 6s 30ms/step - loss: 0.0675 - val_loss: 0.0680\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0672 - val_loss: 0.0678\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0670 - val_loss: 0.0676\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0669 - val_loss: 0.0675\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0667 - val_loss: 0.0673\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0665 - val_loss: 0.0672\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0664 - val_loss: 0.0670\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0662 - val_loss: 0.0668\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0661 - val_loss: 0.0667\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0660 - val_loss: 0.0666\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0659 - val_loss: 0.0665\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 5s 26ms/step - loss: 0.0658 - val_loss: 0.0664\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0656 - val_loss: 0.0663\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0655 - val_loss: 0.0662\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0655 - val_loss: 0.0661\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0654 - val_loss: 0.0660\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 5s 28ms/step - loss: 0.0653 - val_loss: 0.0659\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0652 - val_loss: 0.0659\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0651 - val_loss: 0.0658\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0651 - val_loss: 0.0658\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0650 - val_loss: 0.0657\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0649 - val_loss: 0.0656\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0649 - val_loss: 0.0656\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0648 - val_loss: 0.0655\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0648 - val_loss: 0.0655\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0647 - val_loss: 0.0654\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0647 - val_loss: 0.0654\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0646 - val_loss: 0.0654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Split the full training set into a smaller training set and a validation set\n",
        "x_train, x_valid = x_train_full[:50000] , x_train_full[50000:] \n",
        "y_train, y_valid = y_train_full[:50000], y_train_full[50000:]\n",
        "\n",
        "# Load MNIST data and normalize it\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_valid = x_valid.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Flatten the images to a 784-dimensional vector\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_valid = x_valid.reshape((len(x_valid), np.prod(x_valid.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# Define the autoencoder model\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(256, activation='tanh',use_bias=True)(input_img)\n",
        "decoded = Dense(784, activation='sigmoid',use_bias=True)(encoded)\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the autoencoder on MNIST data for 50 epochs with batch size of 256\n",
        "history = autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_valid, x_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa0BEpUs48no",
        "outputId": "7862980a-0ee7-4b91-d492-d92872826a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "196/196 [==============================] - 5s 22ms/step - loss: 0.2265 - val_loss: 0.1578\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.1359 - val_loss: 0.1181\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.1084 - val_loss: 0.1002\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0950 - val_loss: 0.0908\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0876 - val_loss: 0.0853\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0830 - val_loss: 0.0816\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 5s 26ms/step - loss: 0.0798 - val_loss: 0.0790\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0775 - val_loss: 0.0771\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0758 - val_loss: 0.0757\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 5s 28ms/step - loss: 0.0744 - val_loss: 0.0744\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0733 - val_loss: 0.0735\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0724 - val_loss: 0.0726\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 5s 28ms/step - loss: 0.0717 - val_loss: 0.0719\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0710 - val_loss: 0.0713\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0704 - val_loss: 0.0708\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0699 - val_loss: 0.0704\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0695 - val_loss: 0.0699\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0691 - val_loss: 0.0695\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0687 - val_loss: 0.0692\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0684 - val_loss: 0.0689\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 7s 35ms/step - loss: 0.0681 - val_loss: 0.0686\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0679 - val_loss: 0.0684\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0676 - val_loss: 0.0682\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0674 - val_loss: 0.0679\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0672 - val_loss: 0.0678\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 5s 26ms/step - loss: 0.0670 - val_loss: 0.0676\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0668 - val_loss: 0.0674\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0667 - val_loss: 0.0672\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0665 - val_loss: 0.0671\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0663 - val_loss: 0.0670\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.0662 - val_loss: 0.0669\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0661 - val_loss: 0.0667\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0659 - val_loss: 0.0666\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0658 - val_loss: 0.0665\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.0657 - val_loss: 0.0664\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 6s 28ms/step - loss: 0.0656 - val_loss: 0.0663\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0655 - val_loss: 0.0661\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 4s 22ms/step - loss: 0.0654 - val_loss: 0.0661\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 6s 30ms/step - loss: 0.0653 - val_loss: 0.0660\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0653 - val_loss: 0.0659\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0652 - val_loss: 0.0659\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0651 - val_loss: 0.0658\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0650 - val_loss: 0.0657\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 6s 30ms/step - loss: 0.0650 - val_loss: 0.0657\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0649 - val_loss: 0.0656\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0649 - val_loss: 0.0655\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 6s 30ms/step - loss: 0.0648 - val_loss: 0.0655\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 5s 23ms/step - loss: 0.0648 - val_loss: 0.0654\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.0647 - val_loss: 0.0654\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.0647 - val_loss: 0.0653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 c)\n"
      ],
      "metadata": {
        "id": "Y_r286XSCahW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Split the full training set into a smaller training set and a validation set\n",
        "x_train, x_valid = x_train_full[:50000] , x_train_full[50000:] \n",
        "y_train, y_valid = y_train_full[:50000], y_train_full[50000:]\n",
        "\n",
        "# Load MNIST data and normalize it\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_valid = x_valid.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Flatten the images to a 784-dimensional vector\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_valid = x_valid.reshape((len(x_valid), np.prod(x_valid.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# Define autoencoder architecture\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(256, activation='relu')(input_img)\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "# Define encoder model (to be used as feature extractor)\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# Freeze encoder weights\n",
        "for layer in encoder.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define classification model\n",
        "inputs = Input(shape=(256,))\n",
        "outputs = Dense(10, activation='softmax')(inputs)\n",
        "classifier = Model(inputs, outputs)\n",
        "\n",
        "# Compile models\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train autoencoder\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=10,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_valid, x_valid))\n",
        "\n",
        "# Extract encoded features from training and test data\n",
        "encoded_train = encoder.predict(x_train)\n",
        "encoded_test = encoder.predict(x_test)\n",
        "\n",
        "# Train classification model on encoded features\n",
        "classifier.fit(encoded_train, tf.keras.utils.to_categorical(y_train),\n",
        "               epochs=10,\n",
        "               batch_size=256,\n",
        "               shuffle=True,\n",
        "               validation_data=(encoded_test, tf.keras.utils.to_categorical(y_test)))\n",
        "\n",
        "# Evaluate classification model on test set\n",
        "test_loss, test_acc = classifier.evaluate(encoded_test, tf.keras.utils.to_categorical(y_test))\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ypWQcu684ny",
        "outputId": "48972222-78f4-433e-ba2b-152bb1dd2a08"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "196/196 [==============================] - 4s 18ms/step - loss: 0.3068 - val_loss: 0.2085\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 4s 19ms/step - loss: 0.1848 - val_loss: 0.1651\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 4s 19ms/step - loss: 0.1541 - val_loss: 0.1436\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.1370 - val_loss: 0.1302\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 4s 18ms/step - loss: 0.1258 - val_loss: 0.1210\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 5s 27ms/step - loss: 0.1178 - val_loss: 0.1143\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 6s 29ms/step - loss: 0.1119 - val_loss: 0.1092\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.1072 - val_loss: 0.1052\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 4s 18ms/step - loss: 0.1036 - val_loss: 0.1020\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.1006 - val_loss: 0.0993\n",
            "1563/1563 [==============================] - 3s 2ms/step\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "Epoch 1/10\n",
            "196/196 [==============================] - 2s 6ms/step - loss: 1.5330 - accuracy: 0.6434 - val_loss: 1.0053 - val_accuracy: 0.8188\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.8445 - accuracy: 0.8267 - val_loss: 0.6853 - val_accuracy: 0.8564\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.8532 - val_loss: 0.5597 - val_accuracy: 0.8700\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.5493 - accuracy: 0.8655 - val_loss: 0.4913 - val_accuracy: 0.8798\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.4938 - accuracy: 0.8743 - val_loss: 0.4482 - val_accuracy: 0.8869\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.4561 - accuracy: 0.8810 - val_loss: 0.4175 - val_accuracy: 0.8907\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.4286 - accuracy: 0.8856 - val_loss: 0.3950 - val_accuracy: 0.8961\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.4074 - accuracy: 0.8902 - val_loss: 0.3779 - val_accuracy: 0.8990\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.3904 - accuracy: 0.8935 - val_loss: 0.3643 - val_accuracy: 0.9025\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.3765 - accuracy: 0.8967 - val_loss: 0.3524 - val_accuracy: 0.9050\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3524 - accuracy: 0.9050\n",
            "Test accuracy: 0.9049999713897705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Split the full training set into a smaller training set and a validation set\n",
        "x_train, x_valid = x_train_full[:50000] , x_train_full[50000:] \n",
        "y_train, y_valid = y_train_full[:50000], y_train_full[50000:]\n",
        "\n",
        "# Load MNIST data and normalize it\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_valid = x_valid.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Flatten the images to a 784-dimensional vector\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_valid = x_valid.reshape((len(x_valid), np.prod(x_valid.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# Define autoencoder architecture\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(256, activation='tanh')(input_img)\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "# Define encoder model (to be used as feature extractor)\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# Freeze encoder weights\n",
        "for layer in encoder.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define classification model\n",
        "inputs = Input(shape=(256,))\n",
        "outputs = Dense(10, activation='softmax')(inputs)\n",
        "classifier = Model(inputs, outputs)\n",
        "\n",
        "# Compile models\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train autoencoder\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=10,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_valid, x_valid))\n",
        "\n",
        "# Extract encoded features from training and test data\n",
        "encoded_train = encoder.predict(x_train)\n",
        "encoded_test = encoder.predict(x_test)\n",
        "\n",
        "# Train classification model on encoded features\n",
        "classifier.fit(encoded_train, tf.keras.utils.to_categorical(y_train),\n",
        "               epochs=10,\n",
        "               batch_size=256,\n",
        "               shuffle=True,\n",
        "               validation_data=(encoded_test, tf.keras.utils.to_categorical(y_test)))\n",
        "\n",
        "# Evaluate classification model on test set\n",
        "test_loss, test_acc = classifier.evaluate(encoded_test, tf.keras.utils.to_categorical(y_test))\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyjkHnppBgzI",
        "outputId": "2db91bbf-f4a7-4cd4-c603-278abe3ec371"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "196/196 [==============================] - 4s 19ms/step - loss: 0.2796 - val_loss: 0.1792\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.1583 - val_loss: 0.1400\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 4s 18ms/step - loss: 0.1320 - val_loss: 0.1223\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.1183 - val_loss: 0.1119\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.1096 - val_loss: 0.1049\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 3s 18ms/step - loss: 0.1035 - val_loss: 0.0998\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0989 - val_loss: 0.0958\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 4s 18ms/step - loss: 0.0953 - val_loss: 0.0928\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 4s 21ms/step - loss: 0.0923 - val_loss: 0.0902\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 3s 16ms/step - loss: 0.0898 - val_loss: 0.0880\n",
            "1563/1563 [==============================] - 4s 3ms/step\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "Epoch 1/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 1.3068 - accuracy: 0.6768 - val_loss: 0.7723 - val_accuracy: 0.8338\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.8399 - val_loss: 0.5513 - val_accuracy: 0.8661\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.8630 - val_loss: 0.4688 - val_accuracy: 0.8790\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 0s 2ms/step - loss: 0.4710 - accuracy: 0.8744 - val_loss: 0.4265 - val_accuracy: 0.8861\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 0s 2ms/step - loss: 0.4353 - accuracy: 0.8811 - val_loss: 0.3992 - val_accuracy: 0.8923\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.4113 - accuracy: 0.8860 - val_loss: 0.3802 - val_accuracy: 0.8942\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.8895 - val_loss: 0.3666 - val_accuracy: 0.8974\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 1s 3ms/step - loss: 0.3808 - accuracy: 0.8931 - val_loss: 0.3562 - val_accuracy: 0.9004\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8965 - val_loss: 0.3492 - val_accuracy: 0.9010\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 0s 3ms/step - loss: 0.3617 - accuracy: 0.8983 - val_loss: 0.3404 - val_accuracy: 0.9040\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3404 - accuracy: 0.9040\n",
            "Test accuracy: 0.9039999842643738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)\n"
      ],
      "metadata": {
        "id": "5Er-65dVCyK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Split the full training set into a smaller training set and a validation set\n",
        "x_train, x_valid = x_train_full[:50000] , x_train_full[50000:] \n",
        "y_train, y_valid = y_train_full[:50000], y_train_full[50000:]\n",
        "\n",
        "# Load MNIST data and normalize it\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_valid = x_valid.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Flatten the images to a 784-dimensional vector\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_valid = x_valid.reshape((len(x_valid), np.prod(x_valid.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# Define encoder model\n",
        "inputs = Input(shape=(784,))\n",
        "z = Dense(256, activation='relu')(inputs)\n",
        "encoder = Model(inputs, z)\n",
        "\n",
        "# Define decoder model\n",
        "latent_inputs = Input(shape=(256,))\n",
        "outputs = Dense(784, activation='sigmoid')(latent_inputs)\n",
        "decoder = Model(latent_inputs, outputs)\n",
        "\n",
        "# Define autoencoder model\n",
        "ae_inputs = Input(shape=(784,))\n",
        "ae_encoder = encoder(ae_inputs)\n",
        "ae_decoder = decoder(ae_encoder)\n",
        "autoencoder = Model(ae_inputs, ae_decoder)\n",
        "\n",
        "# Freeze the weights of the encoder\n",
        "for layer in encoder.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define classification model\n",
        "c_inputs = Input(shape=(256,))\n",
        "x = Dense(128, activation='relu')(c_inputs)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "c_outputs = Dense(10, activation='softmax')(x)\n",
        "classifier = Model(c_inputs, c_outputs)\n",
        "\n",
        "# Define joint model\n",
        "joint_inputs = Input(shape=(784,))\n",
        "joint_encoder = encoder(joint_inputs)\n",
        "joint_decoder = decoder(joint_encoder)\n",
        "joint_classifier = classifier(joint_encoder)\n",
        "joint_model = Model(joint_inputs, [joint_decoder, joint_classifier])\n",
        "\n",
        "# Compile models\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')\n",
        "classifier.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "joint_model.compile(optimizer=Adam(learning_rate=0.001), loss=['binary_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 1], metrics=['accuracy'])\n",
        "\n",
        "# Train models\n",
        "autoencoder.fit(x_train, x_train, epochs=10, batch_size=128, validation_data=(x_valid, x_valid))\n",
        "classifier.fit(encoder.predict(x_train), tf.keras.utils.to_categorical(y_train), epochs=10, batch_size=128, validation_data=(encoder.predict(x_valid), tf.keras.utils.to_categorical(y_valid)))\n",
        "joint_model.fit(x_train, [x_train, tf.keras.utils.to_categorical(y_train)], epochs=10, batch_size=128, validation_data=(x_test, [x_test, tf.keras.utils.to_categorical(y_test)]))\n",
        "\n",
        "\n",
        "# Evaluate autoencoder on test set\n",
        "autoencoder_loss = autoencoder.evaluate(x_test, x_test, verbose=0)\n",
        "print('Autoencoder test loss:', autoencoder_loss)\n",
        "\n",
        "# Evaluate classifier on test set\n",
        "classifier_eval = classifier.evaluate(encoder.predict(x_test), tf.keras.utils.to_categorical(y_test), verbose=0)\n",
        "print('Classifier test loss:', classifier_eval[0])\n",
        "print('Classifier test accuracy:', classifier_eval[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "gswYr2tM9zBJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e6aff1-dbd1-4588-f8f7-ad7bcc2d0592"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 6s 14ms/step - loss: 0.2464 - val_loss: 0.1681\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.1493 - val_loss: 0.1336\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.1254 - val_loss: 0.1175\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 5s 14ms/step - loss: 0.1129 - val_loss: 0.1081\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.1051 - val_loss: 0.1020\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.0998 - val_loss: 0.0977\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 5s 14ms/step - loss: 0.0960 - val_loss: 0.0945\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.0932 - val_loss: 0.0922\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0910 - val_loss: 0.0903\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.0893 - val_loss: 0.0889\n",
            "1563/1563 [==============================] - 3s 2ms/step\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 4s 6ms/step - loss: 0.6443 - accuracy: 0.8097 - val_loss: 0.2697 - val_accuracy: 0.9241\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.2874 - accuracy: 0.9142 - val_loss: 0.1997 - val_accuracy: 0.9419\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.2219 - accuracy: 0.9325 - val_loss: 0.1634 - val_accuracy: 0.9523\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.1837 - accuracy: 0.9439 - val_loss: 0.1509 - val_accuracy: 0.9549\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.1574 - accuracy: 0.9528 - val_loss: 0.1396 - val_accuracy: 0.9577\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1397 - accuracy: 0.9570 - val_loss: 0.1223 - val_accuracy: 0.9618\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1266 - accuracy: 0.9615 - val_loss: 0.1176 - val_accuracy: 0.9640\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.1144 - accuracy: 0.9641 - val_loss: 0.1124 - val_accuracy: 0.9649\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.1041 - accuracy: 0.9673 - val_loss: 0.1147 - val_accuracy: 0.9642\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.0945 - accuracy: 0.9700 - val_loss: 0.1059 - val_accuracy: 0.9661\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 8s 17ms/step - loss: 0.1776 - model_34_loss: 0.0878 - model_36_loss: 0.0897 - model_34_accuracy: 0.0159 - model_36_accuracy: 0.9711 - val_loss: 0.1855 - val_model_34_loss: 0.0862 - val_model_36_loss: 0.0992 - val_model_34_accuracy: 0.0139 - val_model_36_accuracy: 0.9678\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.1691 - model_34_loss: 0.0865 - model_36_loss: 0.0826 - model_34_accuracy: 0.0157 - model_36_accuracy: 0.9733 - val_loss: 0.1835 - val_model_34_loss: 0.0852 - val_model_36_loss: 0.0984 - val_model_34_accuracy: 0.0142 - val_model_36_accuracy: 0.9689\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.1642 - model_34_loss: 0.0855 - model_36_loss: 0.0787 - model_34_accuracy: 0.0160 - model_36_accuracy: 0.9745 - val_loss: 0.1826 - val_model_34_loss: 0.0843 - val_model_36_loss: 0.0983 - val_model_34_accuracy: 0.0143 - val_model_36_accuracy: 0.9702\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.1560 - model_34_loss: 0.0847 - model_36_loss: 0.0712 - model_34_accuracy: 0.0156 - model_36_accuracy: 0.9775 - val_loss: 0.1802 - val_model_34_loss: 0.0837 - val_model_36_loss: 0.0965 - val_model_34_accuracy: 0.0146 - val_model_36_accuracy: 0.9708\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1510 - model_34_loss: 0.0841 - model_36_loss: 0.0670 - model_34_accuracy: 0.0156 - model_36_accuracy: 0.9781 - val_loss: 0.1838 - val_model_34_loss: 0.0832 - val_model_36_loss: 0.1006 - val_model_34_accuracy: 0.0141 - val_model_36_accuracy: 0.9701\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1503 - model_34_loss: 0.0836 - model_36_loss: 0.0668 - model_34_accuracy: 0.0159 - model_36_accuracy: 0.9778 - val_loss: 0.1817 - val_model_34_loss: 0.0828 - val_model_36_loss: 0.0989 - val_model_34_accuracy: 0.0137 - val_model_36_accuracy: 0.9702\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.1451 - model_34_loss: 0.0832 - model_36_loss: 0.0620 - model_34_accuracy: 0.0158 - model_36_accuracy: 0.9790 - val_loss: 0.1830 - val_model_34_loss: 0.0824 - val_model_36_loss: 0.1006 - val_model_34_accuracy: 0.0141 - val_model_36_accuracy: 0.9696\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 6s 17ms/step - loss: 0.1409 - model_34_loss: 0.0828 - model_36_loss: 0.0581 - model_34_accuracy: 0.0161 - model_36_accuracy: 0.9806 - val_loss: 0.1830 - val_model_34_loss: 0.0821 - val_model_36_loss: 0.1009 - val_model_34_accuracy: 0.0139 - val_model_36_accuracy: 0.9694\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.1403 - model_34_loss: 0.0825 - model_36_loss: 0.0578 - model_34_accuracy: 0.0158 - model_36_accuracy: 0.9810 - val_loss: 0.1773 - val_model_34_loss: 0.0818 - val_model_36_loss: 0.0954 - val_model_34_accuracy: 0.0133 - val_model_36_accuracy: 0.9709\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.1372 - model_34_loss: 0.0822 - model_36_loss: 0.0550 - model_34_accuracy: 0.0158 - model_36_accuracy: 0.9815 - val_loss: 0.1819 - val_model_34_loss: 0.0816 - val_model_36_loss: 0.1003 - val_model_34_accuracy: 0.0146 - val_model_36_accuracy: 0.9709\n",
            "Autoencoder test loss: 0.08160661906003952\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "Classifier test loss: 0.1002669408917427\n",
            "Classifier test accuracy: 0.9708999991416931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance on test set\n",
        "test_loss, test_autoencoder_loss, test_classifier_loss, test_autoencoder_acc, test_classifier_acc = joint_model.evaluate(x_test, [x_test, tf.keras.utils.to_categorical(y_test)], verbose=0)\n",
        "\n",
        "print('Test set performance:')\n",
        "print(f'Autoencoder loss: {test_autoencoder_loss:.4f}, Classifier loss: {test_classifier_loss:.4f}, Joint loss: {test_loss:.4f}')\n",
        "print(f'Classifier accuracy: {test_classifier_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4XLovw7Fevq",
        "outputId": "76946e8c-cbc6-46a3-da63-66d4005773c4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set performance:\n",
            "Autoencoder loss: 0.0816, Classifier loss: 0.1003, Joint loss: 0.1819\n",
            "Classifier accuracy: 0.9709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import np_utils\n",
        "\n",
        "num_pixels = 784\n",
        "model = Sequential()\n",
        "model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n"
      ],
      "metadata": {
        "id": "ahVvr9M-OBHP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5puY_QilOy0p",
        "outputId": "401b7406-e402-4e31-a249-23456ba5bdb1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_57 (Dense)            (None, 784)               615440    \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 10)                7850      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 623,290\n",
            "Trainable params: 623,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oC9gtF5lOHjW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}